\chapter{Deep Network}
\section{Deep learning}
Deep learning is a part of machine learning. It includes the methods based on learning data representation by allowing the computation on the models that are composed of multiple layers. Each layer extracts the representaion of the input data from the previous layer and computes a new presentation as the input for the next layer. In the hierachy of a model, the higher layers of representation enlarge aspects of the input that is important for discrimination and suppress irrelevant variations. Each level of representations is corresponding to the different level of abstraction. Deep learning methods work on a large dataset using the backpropagation algorithm to improve the result after each step. The methods of deep learning have effectively improved the results in classification problems, object recognition, speech recognition and other domains. 

In deep learning, a neural network is known as the popular method. This is a computing-system based on a collection of connected units (called neurons). Each connection (called synapse) between the neurons can transmit the signal from a neuron to another neuron. The receiving neuron processes the signal that it received, then it sends the resulting signal to another neuron connected to it. Neurons and synaptes may have the weights as learnabled variables, which can used to increase or decrease the strength of signal that it sends to next units. Normally, neurons are organized in layers with different kinds of transformation inside. The signal is travelled multiple times from the first layer (input layer) to the last layer (output layer).

\section{Neural network}
\subsection{Neural}
The basic components of the brain is a neuron. For the ordinary man, we have billion neurons in the human nervous system, and they are connected by the billion of synapses. Each neuron receives input signals from its dendrites and procedures output signals along its axon.\\[0.2cm]
In the computational model of a neuron, the signals travel along the axons interact multiplicatively with the dendrites of the other neuron based on the synaptic strength at the synapse. The synaptic strength are learnable and control the strength at influence or inhibitory of one neuron on another. In basic mode, the input signals are summed and compared with a threshold value. If the sum is greater than threshold value, the neuron can fire, sending a spike along its axon. Actually, we have many firing rate (called activation function) at a neuron, and the common choice of activation function is the \textbf{sigmoid funciont $\sigma$}, because it take a real-valued input and squashes it to range between 0 and 1. Fig. \ref{fignneuron} shows the model of a neuron. The left side presents the connections of a neuron in human brain, the right side describes the mathematical model at the neuron.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{images/neurons.png}
	\caption{A drawing of a biological neuron and its mathematical model}
	\label{fignneuron}
\end{figure}

In mathematical model, the popular activation functions maybe used as:
\begin{itemize}
	\item \textbf{Sigmoid function}:
		\begin{equation}
			\sigma(x) = \frac{1}{1+e^{-x}}
		\end{equation}
	\item \textbf{Tanh function}
		\begin{equation}
			tanh(x) = 2\sigma(2x) - 1
		\end{equation}
	\item \textbf{Rectified Linear Unit (ReLU) function}
		\begin{equation}
			f(x) = max(0,x)
		\end{equation}
	\item \textbf{Maxout}:
		\begin{equation}
			f(w^Tx + b) = max({w_1}^Tx + b_1,{w_2}^Tx + b_2)
		\end{equation}
\end{itemize}
\subsection{Neural network}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{images/neural_net}
	\caption{A model of neural networks}
	\label{fignnnetworks}
\end{figure}
The image \ref{fignnnetworks} show a simple model of neural networks. The leftmost layer in this network is called the input layer, the rightmost layer is called the output layer. The neurons within the input layer are called input neurons, the neurons from output layer are called output neurons. When design the network, the input and the output are often straightforward. It means that the neural networks is designed where the output form one layer is used as the input to the next layer, there are no loops in the network, it always feed forward, never feed back (called feedforward networks).\\[0.2cm]
So, the neural network includes many layers are designed as an directred acyclic graph from the intput to the output layer. The output of previous layer is used as the input of the next layer. At each layer excepts the output layer, the output is indicated by a activation function (i.e loss, tanh,...). The size of a neural network can be to compute as the number of neurons, or the number of parameters.
\section{Deep network}
A deep neural network is a neural network with multiple layers between the input and the output layers. These layers are called hidden layers. Each layer tries to find the correct mathematical operator to turn its input into the next layer. The deep neural network forwards the data from the input layer to the output layer without looping back: The network creates the connections of neurons and assigns the ``weight" for each connection. At each layer, the weights and its input are multiplied and return an output to transfer to the next layer. Further, an algorithm is used to adjust the weights so that make certain parameters more influential until it receives the correct mathematical manipulation on all dataset. Fig. \ref{figndeepnetworks} presents a deep neural network with multiple hidden layers between the input and the output.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{images/deep_neural_network}
	\caption{A deep neural network with multiple hidden layers}
	\label{figndeepnetworks}
\end{figure}

As other machine learning model, designing a deep neural network to solve a problem must be specified an optimization procedure, a cost function and a model family. In deep learning, the gradient-based learning is widely applied because it focus on the difference between the linear models and neural networks. In linear models, the solvers used to train the linear models with global convergence guarantees; instead of, neural network uses gradient-based optimization to drive the cost function to a lowest value after each iteration (because neural network is usually trained by using iterative). Along with gradient-based optimization, a cost fucntion and the represetation of the output of model must be chosen.

A cost function of a neural network is used to compute the difference between the real data and the model's outputs. It is usually updated by each iteration of training process (or validation process). In most of case, we use the cross-entropy between the training data an the model's predictions as the cost function. It means that we define a distribution $p(y | x; \theta)$ and we simply use the priciple of maximum likelihood. But sometimes, we mergely predict some statistic of $y$ conditioned on $x$. The total cost functions of a neural network often combines a primary cost function with a regularization term to make the learning algorithm intend to reduce its generation error.

In our mind, we think that we can separate the choice of the cost function and the output units but in fact, they are related together. For example, if we want to use cross-entropy as the cost function, we need to choose the way to represent the output so that the computing is easy and cheapest. Depending on the solved problem, the output units are chosen to fit with it. For example, we can use the Sigmoid function for a binary classification problem; the Linear function for a transformation with no nonlinearity; the Softmax function for a classifier over \textit{n} different classes.
\section{Back propagation}
A feedforward neural network accepts an input \textbf{$x$} as the initial information, then it was propagated up to the hidden units at each layer and finally procedure an output \textbf{$\hat{y}$}. This is called \textit{forward propagation}. During training, forward propagation can continue until the cost is stable. The \textit{back propagation} algorithm receives the information from the cost (lost function) then flows backward through the network to compute the gradient. This process is repreatedly to discover the gradient for updateing the weights in an attempt to minimize the loss function.

The back-propagation in the form of an algorithm is written as followed:
\begin{enumerate}
	\item \textbf{Input} $x$, \textbf{network} with $L$ layers. Set the corresponding activation $a^1$ for the input layer
	\item \textbf{Feedforward}: For each layer $l = 2, 3, \ldots, L $ compute $z^l = w^la^{l-1} + b^l$ and activation $a^l = \sigma(z^l)$
	\item Compute the error vector: $\delta^L = \nabla_{a}C \odot \sigma'(z^L)$
	\item \textbf{Back-propagation the error}: For each layer $l = L-1, L-2,\ldots, 2$ compute $\delta' = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$
	\item \textbf{Ouput}: the gradient of the cost function given by $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ and $\frac{\delta C}{\delta b^l_j} = \delta^l_j$
\end{enumerate}


The term back-propagation refers only to the method for computing the gradient through recursive application of \textbf{chain rule}. For an arbitrary function $f$, the gradient can be computed from its set of variables whose derivative are desired and set of inputs. It means the process at each function can do independent. In the next, we will describe how to compute the gradient of a function $f(x)$ by using Chain Rule of Calculus which is used to compute the derivaives of functions formed by composing other functions whose derivatives are known.
Let consider a simple multiplication function: $f(x,y) = xy$. The partial derivative of this function as followed:
\begin{equation}
	f(x,y) = xy \rightarrow \frac{\partial f}{\partial x} = y \text{ and } \frac{\partial f}{\partial y} = x
\end{equation}

The objective of derivative is indicating the rate of change of the function with respect to the variable surrounding a small region near a particular point. For example, if $x = 4, y = -3 \rightarrow f(x,y) = -12$, then the derivatives on $x, y$ are:
\begin{equation}
	\frac{\partial f}{\partial x} = -3 \text{ and } \frac{\partial f}{\partial y} = 4
\end{equation}
It means that if we increase the values of $x$, the value of function $f$ will be decreased. Otherwise, if we increase the value of $y$, the ouput of function $f$ is also increased.
For an addition function, the derivatives are:
\begin{equation}
	f(x,y) = x + y \rightarrow \frac{\partial f}{\partial x} = 1 \text{ and } \frac{\partial f}{\partial y} = 1
\end{equation}
And for a MAX operation, the gradient is $1$ on the larger input and $0$ on other inputs:
\begin{equation}
	f(x,y) = max(x,y) \rightarrow \frac{\partial f}{\partial x} = 1 (x \geq y) \text{ and } \frac{\partial f}{\partial y} = 0 (y \geq x)
\end{equation}

Consider example in Fig. \ref{figbackex1}, it presents the function $f(x,y,z) = (x + y)z$. Let see how to apply the derivative to compute the backward pass.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{images/back_ex}
	\caption{The computation graph of function $f$}
	\label{figbackex1}
\end{figure}
The \textit{green} values are the input to compute the forward pass. It is very easy to obtain the result: $f(x,y,z) = (x +y)z = (-2 + 5)(-4) = -12$. In the backward pass, the values are computed by applying derivative: At multiply gate, we can easy to compute the gradient of add gate and input $z$ are $-4$ and $3$, respectively. At the add gate, it takes the gradient and multiples it to all of the local gradient for its input. So, the gradient on both $x$ and $y$ are $1 \ast -4 = -4$. Clearly that if we increase the value of $x$ and $y$, the value of add gate will be decreased and it will turn the ouput of multiply gate increase. So, back-propagation can see as gates communicating to each other to make a change on their outputs (decrease or increase) and to make the final value higher.

\section{Data augmentation}
Like other machine learning model, a deep neural network will be better if is is trained with more data. But in practice, we do not always have a large data instead the amount of data is very limitted. A solution for this problem is to create the fake data and add it to the dataset. However, for different tasks, we may be need to apply the different augmentation methods. \textit{For example,} in classification problem, a classifier needs to take a pair of input $x$ and summary it with a single category $y$. This means that the task of a classifier is to be invariant when the input transforms. So, we can generate new $(x,y)$ pairs just by transforming the $x$ inputs in the training set. But this approach is not reality applicable with a \textit{density estimation} task.

Data augmentation has been a particularly effective technique for object recognition problem \cite{}, speech recognition \cite{}. With the operations like translating the images a few pixels some directions, it can often generate the new images. Even the other operations like rotating or scaling the images have also proven effective.

Noise injection can also be as another form of data augmentation \cite{}. For many classification and regression tasks \cite{}, they have proven that the neural network can be improved the robustness when we train them with random noise applied to their input. They are also inserted into the hidden units which can see as an augmentation at multiple levels of consideration. \cite{} shown that this approach can be highly effective provided that the magnitude of the noise is carefully tuned.

Data augmentation is considered as a part of machine learning algorithms. Usually, operations are generally applicable while other operations are specific to one application domain. So, choosing augmentation methods should be thoroughly examined before applying.

\section{Optimization problem}
The optimization for neural networks is a active area, it involves to deep learning in many contexts and one of all is finding the parameters of a neural network that significantly reduce the cost function and improve the accuracy of the algorithm. In the previous section, we have seen how to compute the gradient with back-propagation. They are used to perform the parameters of the network to reduce the cost of the model. There are several appoaches for performing the update. In this section, we present some established and common techniques which have used to optimize the neural networks.

In previous section, we have tried to calculate the gradient of the loss function. Based on that we can compute the best direction along which we should change our weights to guarante the direction of the stepest descent. This process will be repeated to evaluate the gradients and to perform the parameters updating. This procedure is called \textit{Gradient Descent}. The simplest form of this procedure is to change the parameters along the negative direction. Assuming a vector of parameters \textbf{$x$} and the gradient \textbf{$dx$}, the update has the form: 
\begin{equation}
	x = x - learning\_rate \ast dx \text{ (\textbf{learning\_rate} is a fixed constant)}
\end{equation}
A neural network can be trained on a dataset of hundreds of millions of examples. It seems that wasteful to compute the full loss of the network to perform a parameter. So, we can just apply the Gradient descent over \textbf{batches} of training data to achieve the faster convergence. This process is called \textbf{Stochastic Gradient Descent (SGD)}. Using SGD to update training of a minibatch of \textit{m} examples can be described in  Algorithm \ref{sgdalgorithm}.
\begin{algorithm}
	\caption{SGD update at training iteration $k$}
	\label{sgdalgorithm}
	\begin{algorithmic}
		\REQUIRE Learning rate $\epsilon$
		\REQUIRE Initial parameter $\theta$
		\WHILE{stopping criterion not meet}
			\STATE Sample a minibatch of $m$ example from training set $\{ x^1,x^2,\ldots, x^m \}$ with corresponding targets $y^{(i)}$
			\STATE Compute gradient estimate: $\hat{g} \leftarrow + \frac{1}{m} \nabla_\theta \sum_i L(f(x^i;\theta), y^i) $
			\STATE Apply update $\theta \leftarrow \theta - \epsilon \hat{g}$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

In SGD algorithm, the learning rate is an important parameter. In previous, the learning rate is fixed as a constant but in practice, it is necessary to decrease the learning rate over time. Denote the learning rate at iteration $k$ as $\epsilon_k$. It is common to decay the learning rate linearly until iteration $\tau$:
\begin{equation}
	\epsilon_k = (1 - \alpha) \epsilon_0 + \alpha \epsilon_\tau
\end{equation}
Where $\alpha = \frac{k}{\tau}$ and after iteration $\tau$, it is common to leave $\epsilon$ constant. Usually, the learning rate is chosen by trial and error. When using the linear schedule, the parameters to choose are $\epsilon_0$, $\epsilon_\tau$, and $\tau$: $\tau$ parameter prefers to the number of iteration after that the learning rate will be decreased, $\epsilon_\tau$ indicates the dropping of the learning rate, the last problem is how to choose the value for initial learning rate $\epsilon_0$. If the learning is too large, the cost function often increases significantly. Otherwise, if the learning rate is too low, the learning process will be slow and learning may become stuck with a high-cost value. Experience indicates that the learning rate in the first $100$ iterations should be higher than the next iterations. Therefore, setting up the first learning rate along with a decreasing schedule should be considered together to obtain the best result.

Besides SGD, the method of momentum \cite{} is designed to accelerate learning. It accumulates an exponentially decaying moving average of past gradients and continues to move in their direction. Acording that, a variable role of velocity $\upsilon$ is introduced, it presents the direction and speed that the parameters move through the parameter space. The velocity is set to an exponentially decaying average of the negative gradient. The SGD algorithm with momentum is describe in Algorithm \ref{sgdm_algorithm}.

\begin{algorithm}
	\caption{SGD with momentum}
	\label{sgdm_algorithm}
	\begin{algorithmic}
		\REQUIRE Learning rate $\epsilon$, momentum parameter $\alpha$
		\REQUIRE Initial parameter $\theta$, initial velocity $\upsilon$
		\WHILE{stopping criterion not meet}
			\STATE Sample a minibatch of $m$ example from training set $\{ x^1,x^2,\ldots, x^m \}$ with corresponding targets $y^{(i)}$
			\STATE Compute gradient estimate: $\hat{g} \leftarrow + \frac{1}{m} \nabla_\theta \sum_i L(f(x^i;\theta), y^i) $
			\STATE Compute velocity update: $\upsilon \leftarrow \alpha \upsilon - \epsilon \hat{g} $
			\STATE Apply update $\theta \leftarrow \theta + \upsilon$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\textbf{Nesterov Momentum} is another version of the momentum on SGD. The difference between Nesterov momentum and standard momentum is where the gradient is evaluated after applying the current velocity. It looks like we add a correlation factor to the standard momentum. The complete Nesterov momentum is presented in Algorithm \ref{sgdNm_algorithm}.

\begin{algorithm}
	\caption{SGD with momentum}
	\label{sgdNm_algorithm}
	\begin{algorithmic}
		\REQUIRE Learning rate $\epsilon$, momentum parameter $\alpha$
		\REQUIRE Initial parameter $\theta$, initial velocity $\upsilon$
		\WHILE{stopping criterion not meet}
			\STATE Sample a minibatch of $m$ example from training set $\{ x^1,x^2,\ldots, x^m \}$ with corresponding targets $y^{(i)}$
			\STATE Apply interim update: $\tilde{\theta} \leftarrow \theta + \alpha \upsilon$
			\STATE Compute gradient: $\hat{g} \leftarrow + \frac{1}{m} \nabla_{\tilde{\theta}} \sum_i L(f(x^i;\tilde{\theta}), y^i) $
			\STATE Compute velocity update: $\upsilon \leftarrow \alpha \upsilon - \epsilon \hat{g} $
			\STATE Apply update $\theta \leftarrow \theta + \upsilon$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\section{Conclusion}
