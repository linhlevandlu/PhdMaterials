\documentclass[12pt,a4paper]{article}
\usepackage{fullpage}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{color}

\begin{document}
\title{Landmarks prediction on Beetle anatomical by applying Convolutional Neural Network }
\author{LE Van Linh}
\date{\today}
\maketitle
\begin{abstract}
	In morphometric studies, landmarks are regarded as one of important properties to analyze the object's shape. Especially in biology, collecting complete landmarks can give the information of the oganism. From that, the biologists can study the complex interactions between evolution of the oganism and environment factors. Currently, landmarking is most oftne done by manually. It is time-consuming and difficult to reproduce. In the context of this study, we order to investigate the posibiliy of automatic idenfity landmarks on Beetles. The dataset, that we focus, is one of the most common insect of North-Western France, Carabidae (Beetle). To do this work, we propose a convolutional neural network (CNN) to predict the landmarks on the parts of Beetle: pronotum, head, and body. The proposed model will witnessed on the datasets includes $293$ images (for each part) in different sizes: $192 \times 256$ and $96 \times 96$. The experiments will be done in two directions: training the model from scratch and using fine-tuning. During the experiments, the coordinates of automatic landmarks will be evaluated by comparing with the manual coordinates, which are given by the biologists.
\end{abstract}
\section{Introduction}
Morphometric landmarks are important features in biological investigations. They are using to analyze the shape of the organisms. Depending on the organisms, the number of landmarks on their shapes is different. In addition, when we consider the location of the landmarks with the object, most of landmarks are located on the edges (shape), for example, the landmarks on wings of Drosophila fly \cite{sonnenschein2015image}. Besides, we can also see the landmarks which stayed inside the anatomical part, i.e, landmarks on pinna of human ears \cite{cintas2016automatic}. Currently, the landmarks are set manually by the biologsits. This operation is a time-consuming process and difficult to reproduce. Consequently, a process that proposed automatically the coordinates of landmarks could be interested.

A way to automatize landmarks setting is using the image processing techniques. In which, segmentation is a most often step of the methods because the result from segmentation step is very useful for many purposes. Depending on the characteristics of the image, we can choose the suitable methods to finish target work. In some cases, the object of interest is easy to extract and landmarks can be set by applying a lot of very well-known image analysis procedures, i.e. the landmarks on fly wings \cite{palaniswamy2010automatic}. In a previous study \cite{le2017maelab}, we have analyzed two parts of beetle: left and right mandibles. These parts are easy to segment. In that work, we have applied a set of algorithms based on segmentation, image alignment and SIFT \cite{lowe2004distinctive} descriptor to estimate the landmarks on mandibles. 

Unfortunately, the images of other parts are not simply as the mandibles. Besides the main objects, the images have also the different parts, i.e. we have a part of head and the legs in pronotum images. So, they become very noisy. If we would like to segment the object as traditional processes, the process may have consumed a lot of time and difficult to choose a proper method. This is the reason that we turn the landmarking problem on some parts of beetle (i.e, pronotum, head, and body) to a way of analyzing images without the segmentation step. 

As the beetles have not been dissected, their anatomical parts have not been set apart. So image segmentation of each part, as they are still attached to the whole specimen, is problematic and has been given up. Coordinates of manual landmarks for each part have been provided by the biologists and they are considered as the ground truth to evaluate the predicted ones by our methods. Fig.\ref{figintro} shows the parts of beeltes and their manual landmarks what we are looking for. 

\begin{figure}[h!]
\centering
\subfloat{\includegraphics[scale=0.7]{./images/fpronotum}}~~
\subfloat{\includegraphics[scale=0.7]{./images/ftete}}~~
\subfloat{\includegraphics[scale=0.7]{./images/felytre}}
\caption{The dataset images with their manual landmarks.\\
		 \textit{From left to right}: pronotum, head, and body}
\label{figintro}
\end{figure}

To achieve the landmarks prediction, we have proposed a CNN model \cite{lecun2010convolutional} by using Lasagne library \cite{lasagne}. To evaluate the effective of the model, we have applied two scenarios to predict the landmarks on beetles anatomicals: training from the scratch and applying fine-tuning. The experiments are done on two datasets with different size of the images: $192 \times 256$ and $96 \times 96$.
% In the first evaluation, the proposed model has been trained from scratch on the dataset of each part. In the second step, the evaluation has been modified to use a fine-tuning \cite{.} stage.

Our contributions in this study are as follows: In the next section, we present the related works about automatic estimation landmarks on 2D images. In section \ref{secdata}, we describe the process to augment the dataset. The architecture of proposed network will be presented at section \ref{secarch}. All the experiments of our model will be shown in section \ref{secexpr}. It includes the results to evaluate the model and comparison between two working strategy on neural networks.

\section{Related works}
In geometric morphometry, landmarks (or points of interest) are important features to describe a shape. Depending on the difficulty to segment the objects inside the images, setting automatic landmarks can rely on different methods. When segmentation can be applied, Lowe et al. \cite{lowe2004distinctive} have proposed a method to identify the key points in the 2D image. From the detected key points, the method is able to match two images. Palaniswamy et al. \cite{palaniswamy2010automatic} have applied probabilistic Hough Transform to automatically estimate the landmarks in images
of Drosophila wings. In a previous study \cite{le2017maelab}, we have extended
Palaniswamyâ€™s method to detect landmarks automatically on
beetles mandibles with good results. Unfortunately, when the
segmentation is not precise, we have observed that the results
are getting worse. This is why we have turned our work on
Deep Learning algorithms in order to find a suitable solution to
predict the landmarks without any segmentation step.

Deep Learning models are coming from machine learning theory. They have been introduced in the middle of previous century for artificial intelligence applications but they encounter
several problems to take real-world cases. More recently, the
improvement of computing capacities, both in memory size
and computing time with GPU programming has opened new
perspective for Deep Learning. Many deep learning architectures have been proposed to solve the problems of classification \cite{krizhevsky2012imagenet}, image recognition \cite{simonyan2014very}, speech recognition \cite{hinton2012deep,mikolov2011strategies} and language translation \cite{jean2014using}. To implement
the algorithms, many frameworks have been built such as Caffe
\cite{jia2014caffe}, Theano \cite{2016arXiv160502688short}, Tensorflow \cite{abadi2016tensorflow},.... These frameworks help
the users to design their application by re-using already proposed network architectures. In image analysis domain, Deep
Learning, specifically with CNN, can be used to predict the key
points in an image. Yi Sun et al. \cite{sun2013deep} have proposed a cascaded
convolutional network to predict the key points on the human
face. Zhang et al. \cite{zhang2014facial} optimize facial landmarks detection
with a set of related tasks such as head pose estimation, age
estimation, ...Cintas et al. \cite{cintas2016automatic} have introduced a network to
predict the landmarks on human ear images to characterize ear
shape for biometric analysis. In the same way, we have applied CNN computing to predict the landmarks on beetles anatomical parts. The predicted landmarks will be found in two strategies: training the model from scratch and applying a fine-tuning process.
\section{Dataset}
\label{secdata}
The data includes images in three sets of the beetle: pronotum, body, and head (Fig.\ref{figdata}). Each dataset includes 293 color images which are taken with the same camera in the same
condition with a resolution of $3264 \times 2448$. Each image has
the manual landmarks setting by biologists, i.e, pronotum has $8$
manual landmarks. To obtain the datasets in different resolutions of images ($256 \times 192$ and $96 \times 96$), we have applied different methods to down-sampling from the original images. In the first considered resolutions ($256 \times 192$), we have simplied down-sampling from the original resolution. While, in the second considered resolution ($96 \times 96$), the original images have been cropped from the left to obtain a new resolution of $2448 \times 2448$. Then, the cropped images have been down-sampled to the target resolution. Of course, the coordinates of manual landmarks have been also scaled to fit with the new resolutions.

One of the main characteristics of CNN is that it must use
a huge number of data and one can consider that only several
hundreds of images is not enough to feed a CNN. Moreover,
working with small dataset can push us again to the popular problem of overfitting. So, a way to enlarge the dataset size
has to be considered. In image processing, we usually apply
transform procedures (translation, rotation) to generate a new
image. Unluckily the methods to compute features through a
CNN most often are translation and rotation independent. So, we have used another method to augment dataset from the down-sampling images. 

\begin{figure}[h]
	\centering
	\includegraphics[height=4.0in]{images/cnn_newdatasize/data_aug}
	\caption{An example of augmentation data. From an original image, six augmented version have generated.}
	\label{figdataaug}
\end{figure}

The first procedure has been applied to change the value of
each color channel in the original image. According to that,
a constant is added to each channel of original image in each time. Each constant is sampled in a uniform distribution $\in [1,N] $ to obtain a new value caped at $255$. For
example, we can add a constant $c = 10$ to the red channel of
all images in order to generate new images. This operation can
be done for the three color channels (see the first row of Fig. \ref{figdataaug}).

The second procedure splits the channels of RGB images. It means that we separate the channels of RGB into three gray-scale images. The third row of Fig. \ref{figdataaug} shows the values at three individual channels of an image.

At the end of this process, we are able to generate six versions of the same image, the total number of images used to train and to validate is
$260 \times 7 = 1820$ images (six versions and original image). This
has been an efficient way to proceed to the dataset expansion. Fig.\ref{figdataaug} presents two strategies that we used to augment the data: the image at the second row is the original image, three images at the first row are the result of the first procedure (adding a constant to each color channel), and the third color includes the result images of the second procedure (splitting the channels).

\section{The model}
\label{secarch}
The model, what we used to predict the landmarks, receives an image of $1 \times w \times h$ as the input where $w = 256$ or $96$ and $h = 192$ or $96$. The network was constructed from $3$ \textit{``elementary blocks"} following by $3$ full-connected layers. An elementary block is defined as a sequence of convolution $(C_i)$, maximum pooling $(P_i)$ and dropout $(D_i)$ layers. The \textit{depth} of each convolutional layer and the drop probability of each dropout layer in each elementary block are different. While the parameters of pooling layers are kept the same values. The parameters for each layers are as below, the list of values follows the order of elementary blocks:
\begin{itemize}[nosep,label=\footnotesize$\bullet$]
	\item CONV layres:
	\begin{itemize}[nosep]
		\item Number of filters: 32, 64 and 128,
		\item Kernel filters size: $(3 \times 3), (2 \times 2)$, and $(2 \times 2)$
		\item Stride values: $1,1,1$
		\item No padding is used for CONV layers
	\end{itemize}
	\item MAXPOOL layers:
	\begin{itemize}[nosep]
		\item Kernel filters size: $(2 \times 2), (2 \times 2)$, and $(2 \times 2)$
		\item Stride values: $2,2,2$
		\item No padding is used for CONV layers
	\end{itemize}
	\item DROP layers:
	\begin{itemize}
		\item Probability: $0.1, 0.2$ and $0.3$.
	\end{itemize}
\end{itemize}

In the last full-connected layers (FC), the parameters are: FC1
output: $1000$, FC2 output: $1000$, FC3 output: $16$. As usual, a
dropout layer is inserted between FC1 ond FC2 with a probability equal to $0.5$. The output of the last full-connected layer is a set of $16$ values which corresponds to 8 landmarks ($x$ and $y$ coordinates) which we would like to predict. Fig.\ref{cnnnetwork2} illustrate the order of the layers in the network when we apply the model to predict the landmarks on the input of $1 \times 256 \times 192$.

\begin{figure*}[h]
\centering
\includegraphics[scale=0.3]{images/cnn_newdatasize/arch_model}
\caption{{\small{Network architecture using $3$ \textit{elementary blocks}.
  Convolution
  layer in red, pooling in yellow and dropout in green color.}}} 
\label{cnnnetwork2}
\end{figure*}

The parameters of CNN are shown in Table.\ref{model2parameters}.
\begin{table}[h!]
	\centering
	\begin{tabular}{l l l}
	Parameter & Initial value & End value \\ \hline
	Epochs & 5000 &  \\ \hline
	Training batch size & 128 & \\ \hline
	Testing batch size & 128 & \\ \hline
	Learning rate & 0.03 & 0.0001 \\ \hline
	Momentum & 0.9 & 0.9999 \\ \hline
	\end{tabular}
	\caption{The network parameters in proposed model}
	\label{model2parameters}
\end{table}
\section{Experiments}
\label{secexpr}
The model has been experimented on two resolution datasets: $256 \times 192$ and $96 \times 96$. For each resolution dataset, the model was evaluated in two strategies: training from scratch and fine-tuning. To have the predicted landmarks on all images, during the training process, the cross-validation has been applied to select the training data.

The dataset was trained on the model with $5000$ epochs\footnote{An epoch is a single pass through the full training set}. The images are randomly divided into training set and validation set followed the ratio $6:4$ automatically during training step. The learning rate begin at $0.03$ and decreased to $0.00001$. In vice versa, the momentum started at $0.9$ and increasing to $0.999$ at the end of the training process. 

To evaluate the prediction, the distance between predicted and manual landmarks have been computed in all images. Then, the average distance and standard deviation (SD) have been calculated on each landmark. Following sub-sections describe the average distance and SD of each dataset in each part of beetle.
\subsection{Experiment on the first resolution dataset ($256 \times 192$) }
Table \ref{tbl1} shows the average distances and standard deviations on each landmarks of pronotum part. The table shows the results in three experiment processes on pronotum dataset: training from the scratch, fine-tuning with and without freezing some first layers in the model. The minimum distances belong to the first landmark, while worst distances belong to the sixth landmark in both three cases. In other view, when we consider the results of three experiment processes, the distances have been improved by applying fine-tuning: all the average distances and standard deviations in fine-tuning cases are smaller than the same values when we train the model from the scratch,  for example, if we consider the first landmark of head part, the average distance has decreased from $4.002$ to $2.486$. However, we do not see the significant difference  of the fine-tuning processes (with and without freezing some first layers).

	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | c | c | }
			\hline	
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{Fine-tuning without freezing} & \multicolumn{2}{c|}{Fine-tuning with freezing}  \\ \cline{2-7}
	 & Average & SD & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{4.002}} & \color{green}{\textbf{2.5732}} & \color{green}{\textbf{2.486}} & \color{green}{\textbf{1.5448}} & \color{green}{\textbf{2.47}} & \color{green}{\textbf{1.55}}\\ \hline
			LM2 & 4.4831 & 2.7583 & 2.7198 & 1.7822 & 2.67 & 1.75 \\ \hline
			LM3 & 4.2959 & 2.7067 & 2.6523 & 1.8386 & 2.67 & 1.77 \\ \hline
			LM4 & 4.3865 & 3.0563 & 2.7709 & 1.9483 & 2.74 & 1.91\\ \hline
			LM5 & 4.2925 & 2.9086 & 2.4872 & 1.6235 & 2.57 & 1.60\\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{5.3631}} & \color{red}{\textbf{3.4234}} & \color{red}{\textbf{3.0492}} & \color{red}{\textbf{1.991}} & \color{red}{\textbf{3.09}} & \color{red}{\textbf{2.13}}\\ \hline
			LM7 & 4.636 & 2.8426 & 2.6836 & 1.7781 & 2.65 & 1.80\\ \hline
			LM8 & 4.9363 & 3.0801 & 2.8709 & 1.9662 & 2.96 & 1.97\\ \hline
		\end{tabular}		
		\caption{The average distance and standard deviation on pronotum images ($256 \times 192$)}
		\label{tbl1}
	\end{table}
	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | c| c | }
			\hline	
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{Fine-tuning without freezing} & \multicolumn{2}{c|}{Fine-tuning with freezing}  \\ \cline{2-7}
	 & Average & SD & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{3.87}} & \color{green}{\textbf{3.40}} & \color{green}{\textbf{2.34}} & \color{green}{\textbf{3.11}} & \color{green}{\textbf{2.33}} & \color{green}{\textbf{3.10}} \\ \hline
			LM2 & 3.97 & 3.63 & 2.27 & 3.15 & 2.27 & 3.19 \\ \hline
			LM3 & 3.92 & 3.36 & 2.27 & 2.96 & 2.33 & 2.93 \\ \hline
			LM4 & 3.87 & 3.50 & 2.25 & 3.26 & 2.20 & 3.09 \\ \hline
			LM5 & 4.02 & 3.54 & 2.27 & 3.28 & 2.34 & 3.07 \\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{4.84}} & \color{red}{\textbf{3.59}} & \color{red}{\textbf{3.14}} & \color{red}{\textbf{3.47}} & \color{red}{\textbf{3.12}} & \color{red}{\textbf{3.30}}  \\ \hline
			LM7 & 5.21 & 3.76 & 3.14 & 3.40 & 3.10 & 3.47 \\ \hline
			LM8 & 5.47 & 3.89 & 3.29 & 3.36 & 3.28 & 3.43 \\ \hline
			LM9 & 5.27 & 3.67 & 3.42 & 3.09 & 3.30 & 3.06 \\ \hline
			LM10 & 4.07 & 3.45 & 2.49 & 3.05 & 2.45 & 2.75 \\ \hline
			LM11 & 3.99 & 3.34 & 2.30 & 3.06 & 2.31 & 2.97 \\ \hline						
		\end{tabular}
		\caption{The average distance and standard deviation on body (elytra) images ($256 \times 192$)}
		\label{tbl2}
	\end{table}
	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | c | c | }
			\hline
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{Fine-tuning without freezing} & \multicolumn{2}{c|}{Fine-tuning with freezing}  \\ \cline{2-7}
	 & Average & SD & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{5.53}} & \color{green}{\textbf{3.40}} & \color{green}{\textbf{3.03}} & \color{green}{\textbf{1.89}} & \color{green}{\textbf{3.03}} & \color{green}{\textbf{1.88}} \\ \hline
			LM2 & 5.16 & 3.63 & 2.94 & 1.97 & 2.86 & 1.87 \\ \hline
			LM3 & 5.38 & 3.46 & 2.96 & 1.81 & 2.96 & 1.86\\ \hline
			LM4 & 5.03 & 3.58 & 2.88 & 1.92 & 2.79 & 1.91\\ \hline
			LM5 & 4.84 & 3.26 & 2.76 & 1.75 & 2.77 & 1.78\\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{4.45}} & \color{red}{\textbf{3.37}} & \color{red}{\textbf{2.67}} & \color{red}{\textbf{2.02}} & \color{red}{\textbf{2.61}} & \color{red}{\textbf{1.97}} \\ \hline
			LM7 & 4.79 & 3.08 & 2.29 & 1.64 & 2.31 & 1.69 \\ \hline
			LM8 & 4.53 & 3.11 & 2.20 & 1.54 & 2.20 & 1.58 \\ \hline
			LM9 & 5.14 & 3.13 & 2.57 & 1.60 & 2.66 & 1.63 \\ \hline
			LM10 & 5.06 & 3.17 & 2.44 & 1.53 & 2.55 & 1.61 \\ \hline
		\end{tabular}
		\caption{The average distance and standard deviation on head images ($256 \times 192$)}
		\label{tbl3}
	\end{table}
	
Table \ref{tbl2}, and \ref{tbl3} show the comparation on average distances and standard deviations on body, and head part, respectively. The results of these parts are also the same with the result of pronotum part: the distances of fine-tuning processes are more improved than the result when training from the scratch.
\subsection{Experiment on the second resolution dataset ($96 \times 96$) }
	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | }
			\hline	
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{With fine-tuning}  \\ \cline{2-5}
	 & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{1.61}} & \color{green}{\textbf{0.93}} & \color{green}{\textbf{0.83}} & \color{green}{\textbf{0.55}} \\ \hline
			LM2 & 1.84 & 1.12 & 0.92 & 0.73 \\ \hline
			LM3 & 1.62 & 1.00 & 0.88 & 0.62 \\ \hline
			LM4 & 1.88 & 1.21 & 0.86 & 0.64 \\ \hline
			LM5 & 1.76 & 1.10 & 0.90 & 0.65 \\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{2.13}} & \color{red}{\textbf{1.34}} & \color{red}{\textbf{1.00}} & \color{red}{\textbf{0.79}} \\ \hline
			LM7 & 1.61 & 1.02 & 0.81 & 0.64 \\ \hline
			LM8 & 1.98 & 1.27 & 0.94 & 0.82 \\ \hline
		\end{tabular}
		\caption{The average distance and standard deviation on pronotum images ($96 \times 96$)}
		\label{tbl4}
	\end{table}
	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | }
			\hline
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{With fine-tuning}  \\ \cline{2-5}
	 & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{1.50}} & \color{green}{\textbf{1.27}} & \color{green}{\textbf{0.878}} & \color{green}{\textbf{0.77}} \\ \hline
			LM2 & 1.54 & 1.32 & 0.82 & 0.80 \\ \hline
			LM3 & 1.56 & 1.21 & 0.82 & 0.70 \\ \hline
			LM4 & 1.45 & 1.31 & 0.81 & 0.78 \\ \hline
			LM5 & 1.52 & 1.36 & 0.89 & 0.75 \\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{2.09}} & \color{red}{\textbf{1.58}} & \color{red}{\textbf{1.21}} & \color{red}{\textbf{0.94}} \\ \hline
			LM7 & 2.27 & 1.69 & 1.06 & 1.01 \\ \hline
			LM8 & 2.41 & 1.67 & 1.08 & 0.95 \\ \hline
			LM9 & 2.33 & 1.55 & 1.21 & 0.84 \\ \hline
			LM10 & 1.59 & 1.21 & 0.85 & 0.72 \\ \hline
			LM11 & 1.54 & 1.24 & 0.83 & 0.70 \\ \hline
		\end{tabular}		
		\caption{The average distance and standard deviation on body images ($96 \times 96$)}
		\label{tbl5}
	\end{table}
	\begin{table}[htbp]
		\centering
		\begin{tabular}{ | c | c | c | c | c | }
			\hline
			\multicolumn{1}{|c|}{\multirow{2}{*}{Landmarks}} & \multicolumn{2}{c|}{From scratch} &  \multicolumn{2}{c|}{With fine-tuning}  \\ \cline{2-5}
	 & Average & SD & Average & SD \  \\ \hline
			\color{green}{\textbf{LM1}} & \color{green}{\textbf{2.32}} & \color{green}{\textbf{1.40}} & \color{green}{\textbf{0.98}} & \color{green}{\textbf{0.80}} \\ \hline
			LM2 & 2.00 & 1.36 & 0.88 & 0.62 \\ \hline
			LM3 & 2.18 & 1.41 & 1.00 & 0.86 \\ \hline
			LM4 & 1.90 & 1.40 & 0.91 & 0.68 \\ \hline
			LM5 & 1.93 & 1.26 & 1.19 & 0.96 \\ \hline
			\color{red}{\textbf{LM6}} & \color{red}{\textbf{1.63}} & \color{red}{\textbf{1.31}} & \color{red}{\textbf{1.07}} & \color{red}{\textbf{0.85}} \\ \hline
			LM7 & 1.77 & 1.18 & 0.89 & 0.86 \\ \hline
			LM8 & 1.57 & 1.15 & 0.85 & 0.76 \\ \hline
			LM9 & 1.92 & 1.22 & 0.96 & 0.91 \\ \hline
			LM10 & 1.83 & 1.20 & 0.92 & 0.83 \\ \hline
		\end{tabular}
		\caption{The average distance and standard deviation on head images ($96 \times 96$)}
		\label{tbl6}
	\end{table}
	
Table \ref{tbl4}, \ref{tbl5}, and \ref{tbl6} show the average distances and standard deviations on each landmark on pronotum, body, and head part of beelte in the dataset with the size of $96 \times 96$. In this dataset, we have just considered the fine-tuning process without freezing because we do not see the significantly different between two fine-tuning processes in the case of the previous dataset. Like previous case of dataset $192 \times 256$, the average distances and standard deviations with fine-tuning have been improved than the results when training from scratch.

In addition, when we compare the results on two datasets, the distances have been reduced in the case of small resolution ($96 \times 96$). Even in the case of training from the scratch, the distances are still smaller than the results of fine-tuning in the large resolution ($192 \times 256$).  
\section{Conclusions}
In this study, we have proposed a CNN to predict the landmarks on beetle species. The model includes 3-repeated of an elementary block followed by 3 full-connected layers and a dropout layer. Each elementary block includes a convolutional layer, a maximum pooling layer, and a dropout layer.

The model has been trained and tested on two dataset with different size of images ($256 \times 192$ and $96 \times 96$) by applying two stategies: training from scratch and fine-tuning. The results show that the model had a good prediction on beelte species by training from scratch. However, the quality of predictions have been improved when we have applied fine-tuning. In addition, when we consider the size of the input, the model has given the better result with the small size of images than the larger.  

\bibliographystyle{unsrt}
\bibliography{includes/references}

\end{document}


\subsection{Training on three parts of beetle}


Fig.\ref{figlossallparts} shows the losses during training process. At the beginning, the validation loss is always higher than the tranining loss, but from the $2000^{}$ epochs, the training loss begins stable while the validation loss continue to decrease. At the end of training, the losses values are $0.00029$ and $0.00009$ for training and validation, respectively.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{images/all_parts_10000epochs}
	\caption{The losses during training on the images of three parts}
	\label{figlossallparts}
\end{figure}

Fig.\ref{figtestallparts} shows the predicted landmarks on some images in test set.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.53]{images/all_parts_10000epochs_test}
	\caption{The blue points present for the predicted landmarks on the images in test set}
	\label{figtestallparts}
\end{figure}
\subsection{Fine-tuning on pronotum dataset}
The trained model have been continued to fine-tune \cite{yosinski2014transferable} with pronotum dataset. To get all predicted landmarks for the pronotum images, a scenario to choose the test images is executed. For each round, we have chosen 33 images for the test set, the remaining images have been put to training test. Table.\ref{finetuningloss} shows the losses during fine-tuning on different dataset of pronotum images.

\begin{table}[h!]
	\centering
	\begin{tabular}{l l l}
	Round & Training loss & Validation loss \\ \hline
	1 & 0.00019 & 0.00009  \\ \hline
	2 & 0.00018 & 0.00010 \\ \hline
	3 & 0.00018 & 0.00010 \\ \hline
	4 & 0.00019 & 0.00008 \\ \hline
	5 & 0.00019 & 0.00009 \\ \hline
	6 & 0.00018 & 0.00008 \\ \hline
	7 & 0.00019 & 0.00008 \\ \hline
	8 & 0.00018 & 0.00006 \\ \hline
	9 & 0.00018 & 0.00009 \\ \hline
	\end{tabular}
	\caption{The losses during fine-tuning model}
	\label{finetuningloss}
\end{table}

Fig.\ref{figfintuning} shows an example of the losses during fine-tuning and corresponding predicted landmarks on the test set.

\begin{figure}[h!]
\centering
\subfloat[Training losses and validation loss]{\label{model1loss}\includegraphics[width=0.3\textwidth]{./images/pronotum_fine_tuning_v11}}~~
\subfloat[A pronotum with predicted landmarks]{\label{model1test}\includegraphics[width=0.7\textwidth]{./images/pronotum_fine_tuning_v11_test}}
\caption{An result example when fine-tuning the trained model on pronotum dataset}
\label{figfintuning}
\end{figure}
After fine-tuning, the predicted landmarks of all images are provided. To evaluate the effects of fine-tuning, we calculated the distance between the predicted landmarks and corresponding manual landmarks. A statistic on the distance of each landmarks is also computed. 

Table.\ref{tab2} shows the average error distance given by each landmark. The values in \textbf{Distance 1} and \textbf{Distance 2} columns present for the average distance of all landmark when the pronotum images were trained from scratch and fine-tuning, respectively. From the Table. \ref{tab2}, the result from fine-tuning is significantly improved ($\sim 38\%$) 
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|p{2.5cm}|p{2.5cm}|}
\hline
\textbf{$\#$Landmark} & \textbf{Distance 1} & \textbf{Distance 2} \\ \hline
1 & 4.002 & 2.486  \\ \hline
2 & 4.4831 & 2.720  \\ \hline
3 & 4.2959  & 2.652 \\ \hline
4 & 4.3865  & 2.771 \\ \hline
5 & 4.2925  & 2.487 \\ \hline
6 & 5.3631  & 3.049 \\ \hline
7 & 4.636  & 2.684 \\ \hline
8 & 4.9363  & 2.871 \\ \hline
\end{tabular}
\caption{The average error distance per landmark.}
\label{tab2}
\end{center}
\end{table}
